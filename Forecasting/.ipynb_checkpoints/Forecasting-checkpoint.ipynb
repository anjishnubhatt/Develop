{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Algorithm reusable code\n",
    "\n",
    "\n",
    "#### Loading Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Functions to create holiday and discount flag in the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, rolling_windows, lag_periods, holiday_data, discount_data):\n",
    "    # Create additional features\n",
    "    df['is_holiday'] = df['date'].apply(lambda x: 1 if is_holiday(x, holiday_data) else 0)\n",
    "    df['discount_flag'] = df['date'].apply(lambda x: get_discount_flag(x, discount_data))\n",
    "\n",
    "    # Create rolling mean features\n",
    "    for window in rolling_windows:\n",
    "        df[f'rolling_mean_{window}'] = df['sales'].rolling(window).mean()\n",
    "\n",
    "    # Create lag features\n",
    "    for lag in lag_periods:\n",
    "        df[f'lag_{lag}'] = df['sales'].shift(lag)\n",
    "\n",
    "    # Remove rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_holiday(date, holiday_data):\n",
    "    # Function to determine if a given date is a holiday\n",
    "    # Use the holiday_data DataFrame to check if the date is a holiday\n",
    "    holiday_flag = holiday_data[holiday_data['date'] == date]['flag'].values\n",
    "    if len(holiday_flag) > 0:\n",
    "        return bool(holiday_flag[0])\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_discount_flag(date, discount_data):\n",
    "    # Function to determine the discount flag for a given date\n",
    "    # Use the discount_data DataFrame to check if the date has a discount flag\n",
    "    discount_flag = discount_data[discount_data['date'] == date]['discount_flag'].values\n",
    "    if len(discount_flag) > 0:\n",
    "        return discount_flag[0]\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('store_sales.csv')  # Replace with the actual filename and path of your dataset\n",
    "holiday_data = pd.read_csv('holiday_data.csv')\n",
    "discount_data = pd.read_csv('discount_data.csv')  # Replace with the actual filename and path of your discount data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime type\n",
    "data['date'] = pd.to_datetime(data['date'], format='%d-%m-%Y', dayfirst=True)\n",
    "\n",
    "# Specify the rolling mean windows and lag periods\n",
    "rolling_windows = [2, 3, 6, 9, 12, 18]\n",
    "lag_periods = [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# Specify the number of weeks for training and testing\n",
    "num_train_weeks = len(data['date'].unique()) - 8  # Train on all weeks except the last 8\n",
    "num_test_weeks = 8  # Test on the last 8 weeks\n",
    "\n",
    "predictions_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Store dataframe with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df=pd.DataFrame()\n",
    "\n",
    "for store in data['store'].unique():\n",
    "    store_data = data[data['store'] == store].copy()\n",
    "\n",
    "    # Sort the data by date\n",
    "    store_data.sort_values('date', inplace=True)\n",
    "\n",
    "    # inserting the missing dates\n",
    "    min_date = store_data['date'].min()\n",
    "    max_date = store_data['date'].max()\n",
    "    date_range = pd.date_range(min_date, max_date, freq='W-FRI')\n",
    "    missing_dates = set(date_range) - set(store_data['date'])\n",
    "    missing_data = pd.DataFrame({'date': list(missing_dates)})\n",
    "    store_data = pd.concat([store_data, missing_data]).sort_values('date')\n",
    "\n",
    "    # Fill missing sales values with 0\n",
    "    store_data['sales'].fillna(0, inplace=True)\n",
    "\n",
    "    # Create additional features\n",
    "    store_data = create_features(store_data, rolling_windows, lag_periods, holiday_data, discount_data)\n",
    "    store_df = pd.concat([store_df, store_data])\n",
    "    \n",
    "#store_df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training & Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = store_df[['date', 'store', 'sales']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training to determine the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:07:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:07:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:37 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store 100.0:\n",
      "Best Hyperparameters: {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 0.1, 'seasonality_mode': 'additive'}\n",
      "Best MAPE: 0.016885582886475375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort the dataset by date in ascending order\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Determine the cutoff date for splitting train and test sets\n",
    "cutoff_date = df['date'].max() - pd.DateOffset(weeks=8)\n",
    "\n",
    "train_df=pd.DataFrame()\n",
    "test_df=pd.DataFrame()\n",
    "\n",
    "for store in df['store'].unique():\n",
    "    temp = df[df['store'] == store]    \n",
    "    train = temp[temp['date'] <= cutoff_date]\n",
    "    train_df=pd.concat([train,train_df])\n",
    "    test = temp[temp['date'] > cutoff_date]\n",
    "    test_df=pd.concat([test,test_df])\n",
    "\n",
    "# Define the hyperparameter values to try\n",
    "hyperparameters = {\n",
    "    'changepoint_prior_scale': [0.01, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1.0],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "best_mape_per_store = {}\n",
    "\n",
    "merged_dfs = {}\n",
    "\n",
    "# Iterate over the unique stores\n",
    "for store in train_df['store'].unique():\n",
    "    store_train_df = train_df[train_df['store'] == store][['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "    store_test_df = test_df[test_df['store'] == store][['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "\n",
    "    best_mape = float('inf')\n",
    "    best_params = {}\n",
    "\n",
    "    # Iterate over the hyperparameter combinations\n",
    "    for cp_scale in hyperparameters['changepoint_prior_scale']:\n",
    "        for sp_scale in hyperparameters['seasonality_prior_scale']:\n",
    "            for season_mode in hyperparameters['seasonality_mode']:\n",
    "                # Create and fit the Prophet model\n",
    "                model = Prophet(changepoint_prior_scale=cp_scale,\n",
    "                                seasonality_prior_scale=sp_scale,\n",
    "                                seasonality_mode=season_mode)\n",
    "                model.fit(store_train_df)\n",
    "\n",
    "                # Make predictions for the test set\n",
    "                future = model.make_future_dataframe(periods=8, freq='W-FRI')\n",
    "                forecast = model.predict(future)\n",
    "\n",
    "                # Evaluate the model's performance using MAPE\n",
    "                merged_df = forecast.merge(store_test_df, on='ds')\n",
    "                merged_df['error'] = abs(merged_df['yhat'] - merged_df['y']) / merged_df['y']\n",
    "                mape = merged_df['error'].mean()\n",
    "\n",
    "                # Check if this combination of hyperparameters gives a better result\n",
    "                if mape < best_mape:\n",
    "                    best_mape = mape\n",
    "                    best_params = {'changepoint_prior_scale': cp_scale,\n",
    "                                   'seasonality_prior_scale': sp_scale,\n",
    "                                   'seasonality_mode': season_mode}\n",
    "                    best_merged_df = merged_df.copy()\n",
    "\n",
    "    # Store the best hyperparameters and MAPE for the store\n",
    "    best_mape_per_store[store] = {'best_params': best_params, 'best_mape': best_mape}\n",
    "    merged_dfs[store] = best_merged_df\n",
    "\n",
    "# Print the best hyperparameters and MAPE for each store\n",
    "for store, params in best_mape_per_store.items():\n",
    "    merged_df = merged_dfs[store]\n",
    "    merged_df['store'] = store\n",
    "    print(f\"Store {store}:\")\n",
    "    print('Best Hyperparameters:', params['best_params'])\n",
    "    print('Best MAPE:', params['best_mape'])\n",
    "    print()\n",
    "df_prophet=merged_df[['ds','error','store','y','yhat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:07:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:07:37 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  store         sales\n",
      "0 2012-11-02  100.0  4.669403e+07\n",
      "1 2012-11-09  100.0  4.960658e+07\n",
      "2 2012-11-16  100.0  5.217169e+07\n",
      "3 2012-11-23  100.0  5.379849e+07\n",
      "4 2012-11-30  100.0  5.593464e+07\n",
      "5 2012-12-07  100.0  5.992546e+07\n",
      "6 2012-12-14  100.0  6.425177e+07\n",
      "7 2012-12-21  100.0  6.489349e+07\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Find the maximum week in the dataset\n",
    "max_week = df['date'].max()\n",
    "\n",
    "# Create an empty DataFrame to store the forecasts\n",
    "forecast_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the unique stores\n",
    "for store, params in best_mape_per_store.items():\n",
    "    # Extract the best hyperparameters for the store\n",
    "    best_params = params['best_params']\n",
    "\n",
    "    # Filter the dataset for the specific store\n",
    "    store_df = df[df['store'] == store][['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "\n",
    "    # Create and fit the Prophet model with the best hyperparameters\n",
    "    model = Prophet(**best_params)\n",
    "    model.fit(store_df)\n",
    "\n",
    "    # Make predictions for the next 8 weeks\n",
    "    future_dates = pd.date_range(start=max_week + pd.DateOffset(days=1), periods=8, freq='W-FRI')\n",
    "    future = pd.DataFrame({'ds': future_dates})\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Add the store and the forecasted sales to the DataFrame\n",
    "    forecast['store'] = store\n",
    "    forecast_df = forecast_df.append(forecast)\n",
    "\n",
    "# Rename the columns and select the necessary columns\n",
    "forecast_df = forecast_df.rename(columns={'ds': 'date', 'yhat': 'sales'})\n",
    "forecast_df = forecast_df[['date', 'store', 'sales']]\n",
    "\n",
    "# Print the forecast for the next 8 weeks\n",
    "print(forecast_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
