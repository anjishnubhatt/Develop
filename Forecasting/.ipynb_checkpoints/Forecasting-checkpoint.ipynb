{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Algorithm reusable code\n",
    "\n",
    "\n",
    "#### Loading Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Functions to create holiday and discount flag in the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, rolling_windows, lag_periods, holiday_data, discount_data):\n",
    "    # Create additional features\n",
    "    df['is_holiday'] = df['date'].apply(lambda x: 1 if is_holiday(x, holiday_data) else 0)\n",
    "    df['discount_flag'] = df['date'].apply(lambda x: get_discount_flag(x, discount_data))\n",
    "\n",
    "    # Create rolling mean features\n",
    "    for window in rolling_windows:\n",
    "        df[f'rolling_mean_{window}'] = df['sales'].rolling(window).mean()\n",
    "\n",
    "    # Create lag features\n",
    "    for lag in lag_periods:\n",
    "        df[f'lag_{lag}'] = df['sales'].shift(lag)\n",
    "\n",
    "    # Remove rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_holiday(date, holiday_data):\n",
    "    # Function to determine if a given date is a holiday\n",
    "    # Use the holiday_data DataFrame to check if the date is a holiday\n",
    "    holiday_flag = holiday_data[holiday_data['date'] == date]['flag'].values\n",
    "    if len(holiday_flag) > 0:\n",
    "        return bool(holiday_flag[0])\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_discount_flag(date, discount_data):\n",
    "    # Function to determine the discount flag for a given date\n",
    "    # Use the discount_data DataFrame to check if the date has a discount flag\n",
    "    discount_flag = discount_data[discount_data['date'] == date]['discount_flag'].values\n",
    "    if len(discount_flag) > 0:\n",
    "        return discount_flag[0]\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('store_sales.csv')  # Replace with the actual filename and path of your dataset\n",
    "holiday_data = pd.read_csv('holiday_data.csv')\n",
    "discount_data = pd.read_csv('discount_data.csv')  # Replace with the actual filename and path of your discount data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime type\n",
    "data['date'] = pd.to_datetime(data['date'], format='%d-%m-%Y', dayfirst=True)\n",
    "\n",
    "# Specify the rolling mean windows and lag periods\n",
    "rolling_windows = [2, 3, 6, 9, 12, 18]\n",
    "lag_periods = [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# Specify the number of weeks for training and testing\n",
    "num_train_weeks = len(data['date'].unique()) - 8  # Train on all weeks except the last 8\n",
    "num_test_weeks = 8  # Test on the last 8 weeks\n",
    "\n",
    "predictions_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Store dataframe with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df=pd.DataFrame()\n",
    "\n",
    "for store in data['store'].unique():\n",
    "    store_data = data[data['store'] == store].copy()\n",
    "\n",
    "    # Sort the data by date\n",
    "    store_data.sort_values('date', inplace=True)\n",
    "\n",
    "    # inserting the missing dates\n",
    "    min_date = store_data['date'].min()\n",
    "    max_date = store_data['date'].max()\n",
    "    date_range = pd.date_range(min_date, max_date, freq='W-FRI')\n",
    "    missing_dates = set(date_range) - set(store_data['date'])\n",
    "    missing_data = pd.DataFrame({'date': list(missing_dates)})\n",
    "    store_data = pd.concat([store_data, missing_data]).sort_values('date')\n",
    "\n",
    "    # Fill missing sales values with 0\n",
    "    store_data['sales'].fillna(0, inplace=True)\n",
    "\n",
    "    # Create additional features\n",
    "    store_data = create_features(store_data, rolling_windows, lag_periods, holiday_data, discount_data)\n",
    "    store_df = pd.concat([store_df, store_data])\n",
    "    \n",
    "#store_df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store</th>\n",
       "      <th>sales</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>discount_flag</th>\n",
       "      <th>rolling_mean_2</th>\n",
       "      <th>rolling_mean_3</th>\n",
       "      <th>rolling_mean_6</th>\n",
       "      <th>rolling_mean_9</th>\n",
       "      <th>rolling_mean_12</th>\n",
       "      <th>rolling_mean_18</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>lag_6</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>lag_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2010-06-04</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50188543.12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48973022.84</td>\n",
       "      <td>4.768872e+07</td>\n",
       "      <td>4.676743e+07</td>\n",
       "      <td>4.643200e+07</td>\n",
       "      <td>4.645290e+07</td>\n",
       "      <td>4.669804e+07</td>\n",
       "      <td>45120108.06</td>\n",
       "      <td>45330080.20</td>\n",
       "      <td>48503243.52</td>\n",
       "      <td>43705126.71</td>\n",
       "      <td>44734452.56</td>\n",
       "      <td>45183667.08</td>\n",
       "      <td>47365290.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2010-06-11</td>\n",
       "      <td>100.0</td>\n",
       "      <td>47826546.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49007544.92</td>\n",
       "      <td>4.859086e+07</td>\n",
       "      <td>4.745434e+07</td>\n",
       "      <td>4.648325e+07</td>\n",
       "      <td>4.668936e+07</td>\n",
       "      <td>4.659114e+07</td>\n",
       "      <td>47757502.56</td>\n",
       "      <td>45120108.06</td>\n",
       "      <td>45330080.20</td>\n",
       "      <td>48503243.52</td>\n",
       "      <td>43705126.71</td>\n",
       "      <td>44734452.56</td>\n",
       "      <td>45183667.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  store        sales  is_holiday  discount_flag  rolling_mean_2  \\\n",
       "17 2010-06-04  100.0  50188543.12           0              0     48973022.84   \n",
       "18 2010-06-11  100.0  47826546.72           0              0     49007544.92   \n",
       "\n",
       "    rolling_mean_3  rolling_mean_6  rolling_mean_9  rolling_mean_12  \\\n",
       "17    4.768872e+07    4.676743e+07    4.643200e+07     4.645290e+07   \n",
       "18    4.859086e+07    4.745434e+07    4.648325e+07     4.668936e+07   \n",
       "\n",
       "    rolling_mean_18        lag_2        lag_3        lag_4        lag_5  \\\n",
       "17     4.669804e+07  45120108.06  45330080.20  48503243.52  43705126.71   \n",
       "18     4.659114e+07  47757502.56  45120108.06  45330080.20  48503243.52   \n",
       "\n",
       "          lag_6        lag_7        lag_8  \n",
       "17  44734452.56  45183667.08  47365290.44  \n",
       "18  43705126.71  44734452.56  45183667.08  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fbprophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training & Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anjis\\AppData\\Local\\Temp\\ipykernel_11952\\357916059.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ds'] = pd.to_datetime(df['ds'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "seasonality_mode must be \"additive\" or \"multiplicative\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[209], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m param_grid\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_value \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m# Create a Prophet model with the current hyperparameters\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m         model \u001b[38;5;241m=\u001b[39m Prophet(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{param_name: param_value \u001b[38;5;28;01mfor\u001b[39;00m param_name, _ \u001b[38;5;129;01min\u001b[39;00m param_grid\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Fit the model to the training data\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         model\u001b[38;5;241m.\u001b[39mfit(train_data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\prophet\\forecaster.py:142\u001b[0m, in \u001b[0;36mProphet.__init__\u001b[1;34m(self, growth, changepoints, n_changepoints, changepoint_range, yearly_seasonality, weekly_seasonality, daily_seasonality, holidays, seasonality_mode, seasonality_prior_scale, holidays_prior_scale, changepoint_prior_scale, mcmc_samples, interval_width, uncertainty_samples, stan_backend)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_holiday_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_stan_backend(stan_backend)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\prophet\\forecaster.py:194\u001b[0m, in \u001b[0;36mProphet.validate_inputs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_column_name(h, check_holidays\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseasonality_mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madditive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiplicative\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseasonality_mode must be \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditive\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiplicative\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: seasonality_mode must be \"additive\" or \"multiplicative\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Prepare the data for Prophet\n",
    "df = store_df[['date', 'sales']]\n",
    "df.columns = ['ds', 'y']\n",
    "\n",
    "# Convert the 'ds' column to a pandas datetime object\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.01, 0.1, 1],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Initialize variables to store the best model and hyperparameters\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Perform hyperparameter tuning and cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train_index, val_index in tscv.split(df):\n",
    "    train_data = df.iloc[train_index]\n",
    "    val_data = df.iloc[val_index]\n",
    "\n",
    "    for params in param_grid.values():\n",
    "        for param_value in params:\n",
    "            # Create a Prophet model with the current hyperparameters\n",
    "            model = Prophet(**{param_name: param_value for param_name, _ in param_grid.items()})\n",
    "\n",
    "            # Fit the model to the training data\n",
    "            model.fit(train_data)\n",
    "\n",
    "            # Make predictions on the validation data\n",
    "            forecast = model.predict(val_data[['ds']])\n",
    "\n",
    "            # Calculate the evaluation metric (e.g., mean squared error)\n",
    "            # You should replace the metric with your desired evaluation metric\n",
    "            # Here, we calculate the mean squared error\n",
    "            mse = ((forecast['yhat'] - val_data['y']) ** 2).mean()\n",
    "\n",
    "            # Check if the current model has a better score\n",
    "            if mse > best_score:\n",
    "                best_model = model\n",
    "                best_params = {param_name: param_value for param_name, param_value in zip(param_grid.keys(), params)}\n",
    "                best_score = mse\n",
    "\n",
    "# Create future dates for forecasting\n",
    "num_weeks = 8  # Change this value as per your requirement\n",
    "future_dates = best_model.make_future_dataframe(periods=num_weeks, freq='W-FRI')\n",
    "\n",
    "# Make predictions for the future dates\n",
    "forecast = best_model.predict(future_dates)\n",
    "\n",
    "# Extract the relevant columns from the forecast\n",
    "forecast_df = forecast[['ds', 'yhat']]\n",
    "forecast_df.columns = ['forecast_weeks', 'forecast_sales']\n",
    "\n",
    "# Display the forecasted weeks and sales\n",
    "print(forecast_df.tail(num_weeks))\n",
    "\n",
    "# Display the best hyperparameters and score\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each store\n",
    "skipped_store=[]\n",
    "for store in data['store'].unique():\n",
    "    store_data = data[data['store'] == store].copy()\n",
    "\n",
    "    # Sort the data by date\n",
    "    store_data.sort_values('date', inplace=True)\n",
    "\n",
    "    # inserting the missing dates\n",
    "    min_date = store_data['date'].min()\n",
    "    max_date = store_data['date'].max()\n",
    "    date_range = pd.date_range(min_date, max_date, freq='W-FRI')\n",
    "    missing_dates = set(date_range) - set(store_data['date'])\n",
    "    missing_data = pd.DataFrame({'date': list(missing_dates)})\n",
    "    store_data = pd.concat([store_data, missing_data]).sort_values('date')\n",
    "\n",
    "    # Fill missing sales values with 0\n",
    "    store_data['sales'].fillna(0, inplace=True)\n",
    "\n",
    "    # Create additional features\n",
    "    store_data = create_features(store_data, rolling_windows, lag_periods, holiday_data, discount_data)\n",
    "\n",
    "    # Split the data into features (X) and target variable (y)\n",
    "    X = store_data.drop(['store', 'date', 'sales'], axis=1)\n",
    "    y = store_data['sales']\n",
    "    X_train = X.iloc[:num_train_weeks]\n",
    "    y_train = y.iloc[:num_train_weeks]\n",
    "    \n",
    "    if len(X_train) < 2 or len(y_train) < 2:\n",
    "        skipped_store.append(store)\n",
    "        continue\n",
    "\n",
    "    # Create the scaler for feature scaling\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Define the models for time series and regression\n",
    "    models = [\n",
    "        #('time_series_1', SimpleExpSmoothing(endog=y_train)),\n",
    "        ('time_series_2', Prophet()),\n",
    "        ('time_series_3', SARIMAX(endog=y_train)),\n",
    "        ('regression_1', LinearRegression()),\n",
    "        ('regression_2', DecisionTreeRegressor()),\n",
    "        ('regression_3', SVR()),\n",
    "        ('regression_4', KNeighborsRegressor()),\n",
    "        ('regression_5', RandomForestRegressor()),\n",
    "        ('regression_6', MLPRegressor()),\n",
    "        ('regression_7', GaussianProcessRegressor()),\n",
    "        ('regression_8', xgb.XGBRegressor()),\n",
    "        ('regression_9', RandomForestRegressor()),\n",
    "        ('regression_10', LGBMRegressor())\n",
    "    ]\n",
    "\n",
    "    # Define the parameter grids for each model\n",
    "    param_grids = {\n",
    "        'time_series_1': {\n",
    "            \n",
    "        },\n",
    "        'time_series_2': {\n",
    "            \n",
    "        },\n",
    "        'time_series_3': {\n",
    "            'order': [(1, 0, 0), (0, 0, 1)]\n",
    "        },\n",
    "        'regression_1': {\n",
    "            'fit_intercept': [True, False],\n",
    "            'normalize': [True, False]\n",
    "        },\n",
    "        'regression_2': {\n",
    "            'max_depth': [None, 5, 10]\n",
    "        },\n",
    "        'regression_3': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'epsilon': [0.1, 0.01, 0.001]\n",
    "        },\n",
    "        'regression_4': {\n",
    "            'n_neighbors': [3, 5, 7]\n",
    "        },\n",
    "        'regression_5': {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'max_depth': [None, 5, 10]\n",
    "        },\n",
    "        'regression_6': {\n",
    "            'hidden_layer_sizes': [(100,), (50, 50), (20, 20, 20)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': ['adam', 'lbfgs']\n",
    "        },\n",
    "        'regression_7': {\n",
    "            'alpha': [0.1, 1.0, 10.0]\n",
    "        },\n",
    "        'regression_8': {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'max_depth': [3, 6, 9]\n",
    "        },\n",
    "        'regression_9': {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'max_depth': [None, 5, 10]\n",
    "        },\n",
    "        'regression_10': {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'max_depth': [3, 6, 9]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Perform time series forecasting and regression for each model\n",
    "    for model_name, model in models:\n",
    "        \n",
    "            if model_name == 'time_series_2':  # Prophet model\n",
    "                print(f\"Training {model_name} for store {store}...\")\n",
    "\n",
    "                # Create the pipeline for the Prophet model\n",
    "                steps = [\n",
    "                    ('scaler', scaler),\n",
    "                    ('feature_selection', SelectKBest(score_func=f_regression)),\n",
    "                    ('model', model)\n",
    "                ]\n",
    "                pipeline = Pipeline(steps)\n",
    "\n",
    "                # Perform time series cross-validation with grid search\n",
    "                tscv = TimeSeriesSplit(n_splits=5)\n",
    "                grid_search = GridSearchCV(pipeline, param_grids[model_name], scoring='neg_mean_squared_error', cv=tscv)\n",
    "\n",
    "                # Train the model on the training data\n",
    "                grid_search.fit(X_train, y_train)\n",
    "\n",
    "                # Get the best model\n",
    "                best_model = grid_search.best_estimator_\n",
    "\n",
    "                # Perform predictions for the test set\n",
    "                X_test = X.iloc[num_train_weeks:num_train_weeks + num_test_weeks]\n",
    "                predictions = best_model.predict(X_test)\n",
    "\n",
    "                # Append the predictions to the list\n",
    "                for i, prediction in enumerate(predictions):\n",
    "                    predictions_list.append((store, model_name, prediction, X_test.index[i]))\n",
    "\n",
    "                # Update the features and target variable for the next iteration\n",
    "                X = X.append(X_test)\n",
    "                y = y.append(pd.Series(predictions, index=X_test.index))\n",
    "    \n",
    "            else:\n",
    "\n",
    "                print(f\"Training {model_name} for store {store}...\")\n",
    "\n",
    "                # Create the pipeline for each model\n",
    "                steps = [\n",
    "                    ('scaler', scaler),\n",
    "                    ('feature_selection', SelectKBest(score_func=f_regression)),\n",
    "                    ('model', model)\n",
    "                ]\n",
    "                pipeline = Pipeline(steps)\n",
    "\n",
    "                # Perform time series cross-validation with grid search\n",
    "                tscv = TimeSeriesSplit(n_splits=5)\n",
    "                grid_search = GridSearchCV(pipeline, param_grids[model_name], scoring='neg_mean_squared_error', cv=tscv)\n",
    "\n",
    "                # Train the model on the training data\n",
    "\n",
    "                grid_search.fit(X_train, y_train)\n",
    "\n",
    "                # Get the best model\n",
    "                best_model = grid_search.best_estimator_\n",
    "\n",
    "                # Perform predictions for the test set\n",
    "                X_test = X.iloc[num_train_weeks:num_train_weeks + num_test_weeks]\n",
    "                predictions = best_model.predict(X_test)\n",
    "\n",
    "                # Append the predictions to the list\n",
    "                for i, prediction in enumerate(predictions):\n",
    "                    predictions_list.append((store, model_name, prediction, X_test.index[i]))\n",
    "\n",
    "                # Update the features and target variable for the next iteration\n",
    "                X = X.append(X_test)\n",
    "                y = y.append(pd.Series(predictions, index=X_test.index))\n",
    "\n",
    "\n",
    "# Create a DataFrame for the predictions\n",
    "predictions_df = pd.DataFrame(predictions_list, columns=['store', 'model', 'prediction', 'date'])\n",
    "\n",
    "# Calculate the accuracy of the ensemble model on the test set\n",
    "test_set = data[data['week'] > data['week'].max() - num_test_weeks]  # Filter the test set\n",
    "test_set = test_set[['store', 'date', 'sales']]  # Select relevant columns\n",
    "merged_df = pd.merge(predictions_df, test_set, on=['store', 'date'], how='inner')  # Merge predictions and test set\n",
    "mse = mean_squared_error(merged_df['sales'], merged_df['prediction'])\n",
    "accuracy = 1 - (mse / np.var(merged_df['sales']))\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", accuracy)\n",
    "\n",
    "# Sort the merged dataframe by date\n",
    "merged_df.sort_values('date', inplace=True)\n",
    "\n",
    "# Calculate the inverse error for each model\n",
    "merged_df['inverse_error'] = 1 / (merged_df['prediction'] - merged_df['sales']) ** 2\n",
    "\n",
    "# Calculate the contribution of each model to the ensemble prediction\n",
    "model_contributions = merged_df.groupby(['store', 'model'])['inverse_error'].sum()\n",
    "model_contributions = model_contributions.groupby('store').apply(lambda x: x / x.sum())\n",
    "\n",
    "# Sort the contributions in descending order and select the top 3 models\n",
    "top_models = model_contributions.groupby('store').apply(lambda x: x.nlargest(3)).reset_index()\n",
    "\n",
    "# Filter the predictions dataframe based on the top models\n",
    "final_predictions = pd.merge(predictions_df, top_models, on=['store', 'model'], how='inner')\n",
    "\n",
    "# Print the final dataframe with the ensemble prediction\n",
    "print(final_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
