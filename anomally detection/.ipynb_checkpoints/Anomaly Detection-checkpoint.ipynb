{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c11e11",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13a9c5",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a6e7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sales_data.csv')  # Replace 'sales_data.csv' with your actual file name or path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c0dba",
   "metadata": {},
   "source": [
    "# Standard Deviation calculation and normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3eb8b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of sales_amount for each store\n",
    "store_std = data.groupby('store_number')['sales_amount'].std().reset_index()\n",
    "store_std.rename(columns={'sales_amount': 'std_dev'}, inplace=True)\n",
    "\n",
    "# Merge the standard deviation values with the original data\n",
    "data = pd.merge(data, store_std, on='store_number')\n",
    "# Normalize the sales_amount using the store's standard deviation\n",
    "data['sales_amount_normalized'] = (data['sales_amount'] - data['std_dev'].mean()) / data['std_dev'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b21c3a",
   "metadata": {},
   "source": [
    "# Loading the anomally algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "708b125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize anomaly detection models\n",
    "isolation_forest = IsolationForest(contamination=0.05)\n",
    "local_outlier_factor = LocalOutlierFactor(contamination=0.05)\n",
    "one_class_svm = OneClassSVM(nu=0.05)\n",
    "robust_covariance = EllipticEnvelope(contamination=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76f86d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  store_number        date\n",
      "0          100  26-11-2010\n",
      "1          100  10-12-2010\n",
      "2          100  17-12-2010\n",
      "3          100  24-12-2010\n",
      "4          100  25-11-2011\n",
      "5          100  09-12-2011\n",
      "6          100  16-12-2011\n",
      "7          100  23-12-2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anjis\\AppData\\Local\\Temp\\ipykernel_9492\\162561472.py:61: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  anomalous_dates_df = anomalous_dates_df.append(pd.DataFrame({'store_number': store, 'date': store_anomalous_dates}))\n"
     ]
    }
   ],
   "source": [
    "# Create an empty dataframe to store the results\n",
    "anomalous_dates_df = pd.DataFrame(columns=['store_number', 'date'])\n",
    "\n",
    "# Iterate over each store\n",
    "for store in data['store_number'].unique():\n",
    "    # Filter data for the current store\n",
    "    store_data = data[data['store_number'] == store]\n",
    "\n",
    "    # Check for infinity or large values and replace them with NaN\n",
    "    store_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    store_data.dropna(subset=['sales_amount_normalized'], inplace=True)\n",
    "\n",
    "    if store_data.empty:\n",
    "        continue\n",
    "\n",
    "    # Fit the models to the store's normalized sales_amount data\n",
    "    isolation_forest.fit(store_data[['sales_amount_normalized']])\n",
    "    local_outlier_factor.fit(store_data[['sales_amount_normalized']])\n",
    "    one_class_svm.fit(store_data[['sales_amount_normalized']])\n",
    "    robust_covariance.fit(store_data[['sales_amount_normalized']])\n",
    "\n",
    "    # Predict the anomaly scores for each model\n",
    "    scores_if = isolation_forest.decision_function(store_data[['sales_amount_normalized']])\n",
    "    scores_lof = local_outlier_factor.negative_outlier_factor_\n",
    "    scores_ocsvm = one_class_svm.decision_function(store_data[['sales_amount_normalized']])\n",
    "    scores_rc = robust_covariance.decision_function(store_data[['sales_amount_normalized']])\n",
    "\n",
    "    # Ensemble anomaly scores by averaging\n",
    "    ensemble_scores = (scores_if + scores_lof + scores_ocsvm + scores_rc) / 4\n",
    "\n",
    "    # Add the anomalous dates to the dataframe\n",
    "    store_anomalous_dates = store_data[ensemble_scores < 0]['date']\n",
    "    anomalous_dates_df = anomalous_dates_df.append(pd.DataFrame({'store_number': store, 'date': store_anomalous_dates}))\n",
    "\n",
    "# Reset the index of the resulting dataframe\n",
    "anomalous_dates_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the dataframe with anomalous store numbers and dates\n",
    "print(anomalous_dates_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3d360be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\anjis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store_number        date\n",
      "0           100  26-11-2010\n",
      "1           100  10-12-2010\n",
      "2           100  17-12-2010\n",
      "3           100  24-12-2010\n",
      "4           100  31-12-2010\n",
      "5           100  28-01-2011\n",
      "6           100  25-11-2011\n",
      "7           100  09-12-2011\n",
      "8           100  16-12-2011\n",
      "9           100  23-12-2011\n",
      "10          100  27-01-2012\n",
      "11          100  06-04-2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anjis\\AppData\\Local\\Temp\\ipykernel_9492\\2484794652.py:103: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  anomalous_dates_df = anomalous_dates_df.append(pd.DataFrame({'store_number': store, 'date': store_anomalous_dates}))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, ParameterGrid\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('sales_data.csv')  # Replace 'sales_data.csv' with your actual file name or path\n",
    "\n",
    "# Select the relevant columns for anomaly detection (e.g., sales amount, store number, date)\n",
    "selected_columns = ['sales_amount', 'store_number', 'date']\n",
    "data = df[selected_columns]\n",
    "\n",
    "# Calculate the standard deviation of sales_amount for each store\n",
    "store_std = data.groupby('store_number')['sales_amount'].std().reset_index()\n",
    "store_std.rename(columns={'sales_amount': 'std_dev'}, inplace=True)\n",
    "\n",
    "# Normalize the sales_amount using the store's standard deviation\n",
    "data = pd.merge(data, store_std, on='store_number')\n",
    "data['sales_amount_normalized'] = (data['sales_amount'] - data['std_dev']) / data['std_dev']\n",
    "\n",
    "# Initialize anomaly detection models\n",
    "isolation_forest = IsolationForest()\n",
    "local_outlier_factor = LocalOutlierFactor()\n",
    "one_class_svm = OneClassSVM()\n",
    "robust_covariance = EllipticEnvelope()\n",
    "\n",
    "# Define parameter grids for hyperparameter tuning\n",
    "param_grid_if = {'n_estimators': [100, 200, 300], 'contamination': [0.05, 0.1, 0.15]}\n",
    "param_grid_lof = {'n_neighbors': [5, 10, 15], 'contamination': [0.05, 0.1, 0.15]}\n",
    "param_grid_ocsvm = {'nu': [0.05, 0.1, 0.15]}\n",
    "\n",
    "# Create an empty dataframe to store the results\n",
    "anomalous_dates_df = pd.DataFrame(columns=['store_number', 'date'])\n",
    "\n",
    "# Iterate over each store\n",
    "for store in data['store_number'].unique():\n",
    "    # Filter data for the current store\n",
    "    store_data = data[data['store_number'] == store]\n",
    "\n",
    "    # Check for infinity or large values and replace them with NaN\n",
    "    store_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    store_data.dropna(subset=['sales_amount_normalized'], inplace=True)\n",
    "\n",
    "    if store_data.empty:\n",
    "        continue\n",
    "\n",
    "    # Perform hyperparameter tuning using cross-validation for each model\n",
    "    best_score_if = -np.inf\n",
    "    best_params_if = None\n",
    "    for params in ParameterGrid(param_grid_if):\n",
    "        isolation_forest.set_params(**params)\n",
    "        isolation_forest.fit(store_data[['sales_amount_normalized']])  # Fit the model\n",
    "        scores = isolation_forest.decision_function(store_data[['sales_amount_normalized']])\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score > best_score_if:\n",
    "            best_score_if = mean_score\n",
    "            best_params_if = params\n",
    "\n",
    "    best_score_lof = -np.inf\n",
    "    best_params_lof = None\n",
    "    for params in ParameterGrid(param_grid_lof):\n",
    "        local_outlier_factor.set_params(**params)\n",
    "        local_outlier_factor.fit(store_data[['sales_amount_normalized']])  # Fit the model\n",
    "        scores = -local_outlier_factor.negative_outlier_factor_\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score > best_score_lof:\n",
    "            best_score_lof = mean_score\n",
    "            best_params_lof = params\n",
    "\n",
    "    best_score_ocsvm = -np.inf\n",
    "    best_params_ocsvm = None\n",
    "    for params in ParameterGrid(param_grid_ocsvm):\n",
    "        one_class_svm.set_params(**params)\n",
    "        one_class_svm.fit(store_data[['sales_amount_normalized']])  # Fit the model\n",
    "        scores = one_class_svm.decision_function(store_data[['sales_amount_normalized']])\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score > best_score_ocsvm:\n",
    "            best_score_ocsvm = mean_score\n",
    "            best_params_ocsvm = params\n",
    "\n",
    "    # Fit the models to the store's normalized sales_amount data using the best parameters from cross-validation\n",
    "    isolation_forest.set_params(**best_params_if)\n",
    "    local_outlier_factor.set_params(**best_params_lof)\n",
    "    one_class_svm.set_params(**best_params_ocsvm)\n",
    "    isolation_forest.fit(store_data[['sales_amount_normalized']])\n",
    "    local_outlier_factor.fit(store_data[['sales_amount_normalized']])\n",
    "    one_class_svm.fit(store_data[['sales_amount_normalized']])\n",
    "    scores_rc = robust_covariance.fit(store_data[['sales_amount_normalized']]).decision_function(store_data[['sales_amount_normalized']])\n",
    "\n",
    "    # Predict the anomaly scores for each model\n",
    "    scores_if = isolation_forest.decision_function(store_data[['sales_amount_normalized']])\n",
    "    scores_lof = -local_outlier_factor.negative_outlier_factor_\n",
    "    scores_ocsvm = one_class_svm.decision_function(store_data[['sales_amount_normalized']])\n",
    "\n",
    "    # Ensemble anomaly scores by averaging\n",
    "    ensemble_scores = (scores_if + scores_lof + scores_ocsvm + scores_rc) / 4\n",
    "\n",
    "    # Add the anomalous dates to the dataframe\n",
    "    store_anomalous_dates = store_data[ensemble_scores < 0]['date']\n",
    "    anomalous_dates_df = anomalous_dates_df.append(pd.DataFrame({'store_number': store, 'date': store_anomalous_dates}))\n",
    "\n",
    "# Reset the index of the resulting dataframe\n",
    "anomalous_dates_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the dataframe with anomalous store numbers and dates\n",
    "print(anomalous_dates_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f65a72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_dates_df.to_csv('anomalous_dates_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec096c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
